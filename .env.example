# POaaS Environment Configuration
# Copy this file to .env and fill in your values:
#   cp .env.example .env

# ---------- Model Serving ----------
# vLLM server URL (default: http://localhost:8000)
VLLM_URL=http://localhost:8000

# Hugging Face token (required for gated models like Llama)
# Request access at https://huggingface.co/meta-llama
HUGGING_FACE_HUB_TOKEN=your-hf-token-here

# OpenAI API key (optional; only needed if using OpenAI models instead of vLLM)
# OPENAI_API_KEY=your-api-key-here

# ---------- POaaS Settings ----------
# Ablation mode: full | no_skip | no_drift
POAAS_ABLATION=full

# Directory for run artifacts
POAAS_RUNS_DIR=./runs

# ---------- Service URLs (only change for remote deployment) ----------
# CLEANER_URL=http://localhost:8002
# PARAPHRASER_URL=http://localhost:8003
# FACT_ADDER_URL=http://localhost:8004

# ---------- Hugging Face Cache (for Docker) ----------
# HF_CACHE=~/.cache/huggingface
